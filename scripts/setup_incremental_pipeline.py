#!/usr/bin/env python3
"""
Setup automated incremental data pipeline from Snowflake to Neo4j.

This script configures hourly incremental syncs using the Aura Import API.
It creates a pipeline configuration that can be triggered by cron, Airflow, or Cloud Workflows.

Usage:
    python scripts/setup_incremental_pipeline.py \
        --import-model-id fc371c86-7966-40b5-82b3-93f196d0b928 \
        --instance-id 705c1e42 \
        --schedule "0 * * * *"
"""
import argparse
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, Any

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.aura_manager import AuraManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_pipeline_config(
    import_model_id: str,
    instance_id: str,
    schedule: str,
    pipeline_name: str = "snowflake_incremental_sync"
) -> Dict[str, Any]:
    """
    Create pipeline configuration for incremental syncs.

    Args:
        import_model_id: Aura Import model ID (configured in console)
        instance_id: Target Neo4j Aura instance ID
        schedule: Cron schedule (e.g., "0 * * * *" for hourly)
        pipeline_name: Descriptive name for the pipeline

    Returns:
        Pipeline configuration dict
    """
    config = {
        "name": pipeline_name,
        "import_model_id": import_model_id,
        "instance_id": instance_id,
        "schedule": schedule,
        "type": "incremental",
        "enabled": True,
        "retry_policy": {
            "max_retries": 3,
            "retry_delay_seconds": 300
        },
        "timeout_seconds": 3600,
        "notifications": {
            "on_failure": True,
            "on_success": False
        }
    }

    return config


def save_pipeline_config(config: Dict[str, Any], config_dir: Path = Path("config")):
    """Save pipeline configuration to JSON file."""
    config_dir.mkdir(exist_ok=True)
    config_file = config_dir / "import_pipelines.json"

    # Load existing configs if any
    pipelines = []
    if config_file.exists():
        with open(config_file) as f:
            pipelines = json.load(f)

    # Check if pipeline with same name exists
    existing_idx = next(
        (i for i, p in enumerate(pipelines) if p.get("name") == config.get("name")),
        None
    )

    if existing_idx is not None:
        logger.info(f"Updating existing pipeline: {config['name']}")
        pipelines[existing_idx] = config
    else:
        logger.info(f"Adding new pipeline: {config['name']}")
        pipelines.append(config)

    # Save back to file
    with open(config_file, 'w') as f:
        json.dump(pipelines, f, indent=2)

    logger.info(f"Pipeline configuration saved to: {config_file}")

    return config_file


def generate_cron_entry(config: Dict[str, Any], script_path: Path) -> str:
    """
    Generate crontab entry for the pipeline.

    Args:
        config: Pipeline configuration
        script_path: Path to the execution script

    Returns:
        Crontab entry string
    """
    schedule = config["schedule"]
    name = config["name"]

    cron_entry = (
        f"# {name} - Incremental sync from Snowflake to Neo4j\n"
        f"{schedule} cd {script_path.parent.parent} && "
        f"python {script_path} --pipeline-name {name} "
        f">> logs/import_{name}.log 2>&1"
    )

    return cron_entry


def generate_airflow_dag(config: Dict[str, Any]) -> str:
    """
    Generate Airflow DAG for the pipeline.

    Args:
        config: Pipeline configuration

    Returns:
        Python code for Airflow DAG
    """
    name = config["name"]
    import_model_id = config["import_model_id"]
    instance_id = config["instance_id"]
    schedule = config["schedule"]

    dag_code = f'''"""
Airflow DAG for {name}.

Automatically generated by setup_incremental_pipeline.py
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
from pathlib import Path

# Add project to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.aura_manager import AuraManager

default_args = {{
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': {config["retry_policy"]["max_retries"]},
    'retry_delay': timedelta(seconds={config["retry_policy"]["retry_delay_seconds"]}),
}}

dag = DAG(
    '{name}',
    default_args=default_args,
    description='Incremental sync from Snowflake to Neo4j Aura',
    schedule_interval='{schedule}',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['neo4j', 'incremental', 'import'],
)


def run_incremental_import(**context):
    """Execute incremental import job."""
    manager = AuraManager()
    manager.setup_import_client()

    # Trigger import job
    job = manager.create_import_job(
        import_model_id="{import_model_id}",
        db_id="{instance_id}"
    )

    print(f"Import job created: {{job.id}}")

    # Wait for completion
    from src.aura_import_client import print_job_progress
    final_job = manager.wait_for_import_completion(
        job.id,
        poll_interval=30,
        max_wait={config["timeout_seconds"]},
        callback=print_job_progress
    )

    if final_job.state != "Completed":
        raise Exception(f"Import job failed: {{final_job.progress.exit_status_message}}")

    print(f"Import completed successfully: {{final_job.id}}")

    # Push job ID to XCom for downstream tasks
    context['task_instance'].xcom_push(key='import_job_id', value=final_job.id)


import_task = PythonOperator(
    task_id='run_incremental_import',
    python_callable=run_incremental_import,
    dag=dag,
)
'''

    return dag_code


def main():
    parser = argparse.ArgumentParser(
        description="Setup automated incremental data pipeline from Snowflake to Neo4j"
    )
    parser.add_argument(
        "--import-model-id",
        required=True,
        help="Aura Import model ID (from console URL)"
    )
    parser.add_argument(
        "--instance-id",
        required=True,
        help="Target Neo4j Aura instance ID"
    )
    parser.add_argument(
        "--schedule",
        default="0 * * * *",
        help="Cron schedule (default: hourly)"
    )
    parser.add_argument(
        "--name",
        default="snowflake_incremental_sync",
        help="Pipeline name"
    )
    parser.add_argument(
        "--generate-airflow-dag",
        action="store_true",
        help="Generate Airflow DAG file"
    )

    args = parser.parse_args()

    # Verify Aura Manager is configured
    logger.info("Verifying Aura configuration...")
    try:
        manager = AuraManager()
        manager.setup_import_client()
        logger.info("✓ Aura Import API configured")
    except Exception as e:
        logger.error(f"✗ Aura configuration error: {e}")
        logger.error("Please set environment variables: AURA_API_CLIENT_ID, AURA_API_CLIENT_SECRET, "
                    "AURA_ORGANIZATION_ID, AURA_PROJECT_ID")
        sys.exit(1)

    # Create pipeline configuration
    logger.info("Creating pipeline configuration...")
    config = create_pipeline_config(
        import_model_id=args.import_model_id,
        instance_id=args.instance_id,
        schedule=args.schedule,
        pipeline_name=args.name
    )

    # Save configuration
    config_file = save_pipeline_config(config)

    logger.info("\n" + "="*60)
    logger.info("Pipeline Configuration Summary")
    logger.info("="*60)
    logger.info(f"Name: {config['name']}")
    logger.info(f"Import Model ID: {config['import_model_id']}")
    logger.info(f"Target Instance: {config['instance_id']}")
    logger.info(f"Schedule: {config['schedule']}")
    logger.info(f"Config File: {config_file}")
    logger.info("="*60)

    # Generate execution script if it doesn't exist
    script_path = Path("scripts/run_import_pipeline.py")
    if not script_path.exists():
        logger.info(f"\nGenerating execution script: {script_path}")
        # This would create run_import_pipeline.py - see next script

    # Generate cron entry
    logger.info("\n" + "="*60)
    logger.info("Cron Configuration")
    logger.info("="*60)
    cron_entry = generate_cron_entry(config, script_path)
    logger.info("Add this to your crontab (crontab -e):\n")
    logger.info(cron_entry)

    # Generate Airflow DAG if requested
    if args.generate_airflow_dag:
        dag_path = Path(f"dags/{config['name']}_dag.py")
        dag_path.parent.mkdir(exist_ok=True)

        with open(dag_path, 'w') as f:
            f.write(generate_airflow_dag(config))

        logger.info(f"\n✓ Airflow DAG generated: {dag_path}")
        logger.info("Copy this file to your Airflow DAGs folder")

    logger.info("\n" + "="*60)
    logger.info("Next Steps")
    logger.info("="*60)
    logger.info("1. Test the pipeline manually:")
    logger.info(f"   python scripts/run_import_pipeline.py --pipeline-name {config['name']}")
    logger.info("\n2. Schedule with cron (add crontab entry above)")
    logger.info("\n3. OR schedule with Airflow (copy generated DAG)")
    logger.info("\n4. Monitor logs:")
    logger.info(f"   tail -f logs/import_{config['name']}.log")


if __name__ == "__main__":
    main()
