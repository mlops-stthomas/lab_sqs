# Neo4j Aura Import & SQS Integration

Comprehensive data pipeline and ingestion system combining:
- **Neo4j Aura Import API** - Programmatic data loading from Snowflake/BigQuery/S3
- **Aura CLI Integration** - Instance and GraphQL API management
- **AWS SQS** - Message queue for async job processing
- **Automated Pipelines** - Hourly incremental syncs and historical onboarding
- **GPT-5 Responses API** - Context-Free Grammar for Cypher generation

---

## Project Structure

```
lab_sqs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ aura_manager.py              # Unified Aura management (CLI + API)
â”‚   â”œâ”€â”€ consumer.py                  # SQS message consumer
â”‚   â”œâ”€â”€ msg_writer.py                # SQS message writer
â”‚   â”œâ”€â”€ writer.py                    # Message writer utilities
â”‚   â””â”€â”€ settings.py                  # Configuration management
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_incremental_pipeline.py    # Configure automated pipelines
â”‚   â”œâ”€â”€ run_import_pipeline.py           # Execute configured pipelines
â”‚   â”œâ”€â”€ historical_import.py             # One-time bulk import
â”‚   â”œâ”€â”€ check_import_status.py           # Monitor import jobs
â”‚   â””â”€â”€ test_aura_setup.py               # Comprehensive test suite
â”‚
â”œâ”€â”€ lab5_mlflow/
â”‚   â”œâ”€â”€ gepa_runner.py               # GEPA optimizer with Responses API grammar demos
â”‚   â”œâ”€â”€ gepa.py                      # Toastâ†’Neo4j pipeline
â”‚   â”œâ”€â”€ gepa_optimizer.py            # GEPA optimization framework
â”‚   â””â”€â”€ gepa_feedback.py             # Feedback system
â”‚
â”œâ”€â”€ config/                          # Pipeline configurations (auto-generated)
â”œâ”€â”€ logs/                            # Import execution logs
â”œâ”€â”€ reports/                         # Import reports
â”‚
â”œâ”€â”€ .env                             # Environment configuration
â”œâ”€â”€ AURA_SETUP.md                    # Complete Aura setup guide
â”œâ”€â”€ MESSAGE_ORDERING.md              # SQS FIFO message ordering guide
â”œâ”€â”€ SETUP_GUIDE.md                   # SQS setup guide
â””â”€â”€ README.md                        # This file
```

---

## Quick Start

### 1. Install Dependencies

```bash
# Using UV (recommended)
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# Or using pip
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure Environment

Edit `.env` and add your credentials:

```bash
# Aura API (get from console.neo4j.io > Account Settings > API Keys)
AURA_API_CLIENT_ID=MPGDSjrdi1iYhpcFGkmua1LKkTCEMjPx
AURA_API_CLIENT_SECRET=LnaIH0C2BSUxTlXTnrBdZsR0Tbgxi6bbJTA8clk7wlZKe0TAmhmIOmaZuIO1FyYj

# Get these from console.neo4j.io
AURA_ORGANIZATION_ID=your-org-id
AURA_PROJECT_ID=your-project-id

# AWS (for SQS)
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-2

# SQS Queue URLs
QUEUE_URL=https://sqs.us-east-2.amazonaws.com/...
DLQ_URL=https://sqs.us-east-2.amazonaws.com/...
```

### 3. Test Aura Setup

```bash
python scripts/test_aura_setup.py
```

**Expected output:**
```
âœ“ Environment Variables: PASSED
âœ“ Aura CLI: PASSED
âœ“ Health Check: PASSED
âœ“ Import API Authentication: PASSED
ğŸ‰ All tests passed!
```

**See full setup guide**: [AURA_SETUP.md](AURA_SETUP.md)

---

## Features

### 1. Aura Import API Integration âœ…

Programmatic data loading from cloud data warehouses:

```python
from src.aura_manager import AuraManager

manager = AuraManager()
manager.setup_import_client()

# Trigger import job
job = manager.create_import_job(
    import_model_id="your-model-id",
    db_id="705c1e42"
)

# Wait for completion
from src.aura_import_client import print_job_progress
final_job = manager.wait_for_import_completion(
    job.id,
    callback=print_job_progress
)
```

**Features**:
- OAuth 2.0 authentication with token caching
- Idempotent imports (safe to re-run)
- Progress monitoring with callbacks
- Multiple data sources: Snowflake, BigQuery, S3

### 2. Automated Data Pipelines âœ…

**Hourly Incremental Sync**:
```bash
# Configure pipeline
python scripts/setup_incremental_pipeline.py \
  --import-model-id fc371c86-... \
  --instance-id 705c1e42 \
  --schedule "0 * * * *"

# Test run
python scripts/run_import_pipeline.py \
  --pipeline-name snowflake_incremental_sync \
  --dry-run

# Schedule with cron
crontab -e
# Add: 0 * * * * cd /path/to/lab_sqs && python scripts/run_import_pipeline.py ...
```

**Historical Onboarding**:
```bash
python scripts/historical_import.py \
  --import-model-id your-historical-model-id \
  --instance-id 705c1e42 \
  --verify \
  --create-snapshot
```

### 3. GPT-5 Responses API with Cypher Grammar âœ…

Context-Free Grammar for constrained Cypher generation:

```python
# From lab5_mlflow/gepa_runner.py
cypher_grammar = textwrap.dedent(r"""
    start: match_clause (SP where_clause)? SP return_clause
    match_clause: "MATCH" SP pattern
    pattern: node (SP? relationship SP? node)*
    return_clause: "RETURN" SP return_items
""")

tools = [{
    "type": "custom",
    "name": "cypher_grammar",
    "format": {"type": "grammar", "syntax": "lark", "definition": cypher_grammar}
}]

resp = await llm.create_completion(
    messages=[{"role": "user", "content": "Find restaurants with orders over $500"}],
    tools=tools
)
```

**Features**:
- Constrained Cypher generation (MATCH..RETURN patterns)
- MSSQL grammar example
- Verbosity control (low/medium/high)
- Minimal reasoning mode

### 4. SQS Message Queue âœ…

**Start Consumer**:
```bash
python src/consumer.py
```

**Send Messages**:
```bash
# Send 1000 messages
python src/writer.py --n 1000

# Send custom message
python src/msg_writer.py --msg "Hello SQS"
```

**Features**:
- FIFO ordering with message groups
- Dead letter queue for failed messages
- Long polling for efficiency

---

## Available Scripts

| Script | Purpose | Usage |
|--------|---------|-------|
| `test_aura_setup.py` | Test all Aura components | `python scripts/test_aura_setup.py` |
| `setup_incremental_pipeline.py` | Configure automated pipelines | `--import-model-id ID --schedule "0 * * * *"` |
| `run_import_pipeline.py` | Execute pipelines | `--pipeline-name NAME [--dry-run]` |
| `historical_import.py` | One-time bulk import | `--import-model-id ID --verify` |
| `check_import_status.py` | Monitor import jobs | `--job-id ID [--watch]` |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Application Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GEPA Runner  â”‚  â”‚ Incremental  â”‚  â”‚ Historical  â”‚ â”‚
â”‚  â”‚ (GPT-5 CFG)  â”‚  â”‚ Pipelines    â”‚  â”‚ Onboarding  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚    Aura Manager          â”‚
    â”‚   â”‚  - CLI wrapper           â”‚
    â”‚   â”‚  - Import API client     â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚  Aura Import API v2beta1 â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚   Neo4j Aura        â”‚
    â”‚   â”‚  3 Instances:       â”‚
    â”‚   â”‚  - Melting Pot KG   â”‚
    â”‚   â”‚  - Proper KG        â”‚
    â”‚   â”‚  - Tray KG          â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â–²
    â”‚              â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚   Data Sources      â”‚
    â”‚   â”‚  - Snowflake        â”‚
    â”‚   â”‚  - BigQuery         â”‚
    â”‚   â”‚  - S3               â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â””â”€â”€ AWS SQS (async job processing)
```

---

## Documentation

- **[AURA_SETUP.md](AURA_SETUP.md)** - Complete Aura integration setup guide
- **[MESSAGE_ORDERING.md](MESSAGE_ORDERING.md)** - SQS FIFO message ordering
- **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - SQS queue setup

---

## Integration with lab5_mlflow

This project shares components with the parent `lab5_mlflow` project:

**Shared Files**:
- `src/aura_import_client.py` - Aura Import API client
- `src/multi_neo4j_connector.py` - Multi-instance Neo4j connector

**Related Projects**:
- **GEPA Optimizer** - Genetic-Pareto prompt optimization
- **MLFlow Integration** - Model tracking and serving
- **Federated GraphQL** - Unified data access layer

**See**: `/lab5_mlflow/DEPLOYMENT_SUMMARY.md`

---

## Workflow Examples

### Daily Automated Import

```bash
# 1. Create import model in Aura Console for daily data
# 2. Set up pipeline
python scripts/setup_incremental_pipeline.py \
  --import-model-id <model-id> \
  --instance-id 705c1e42 \
  --schedule "0 2 * * *"  # 2 AM daily

# 3. Add to crontab
crontab -e
```

### Historical Onboarding

```bash
# 1. Create import model for all historical data
# 2. Run one-time import
python scripts/historical_import.py \
  --import-model-id <historical-model-id> \
  --instance-id 705c1e42 \
  --knowledge-graph melting-pot \
  --verify \
  --create-snapshot

# 3. Verify in Neo4j Browser
# MATCH (o:Order) RETURN count(o)
```

### Monitoring Imports

```bash
# Check specific job
python scripts/check_import_status.py --job-id <job-id> --progress

# Watch job until completion
python scripts/check_import_status.py --job-id <job-id> --watch

# List all configured pipelines
python scripts/run_import_pipeline.py --list
```

---

## Troubleshooting

### "Environment variables not set"

Check all 4 Aura API variables in `.env`:
- `AURA_API_CLIENT_ID`
- `AURA_API_CLIENT_SECRET`
- `AURA_ORGANIZATION_ID`
- `AURA_PROJECT_ID`

Get Organization and Project IDs from [console.neo4j.io](https://console.neo4j.io)

### "Import job failed: Schema validation error"

1. Verify table names in import model match Snowflake schema
2. Refresh data source if tables changed
3. Test model with sample data in Aura Console

### "Duplicate nodes created"

1. Go to import model settings in Aura Console
2. Add unique constraints on ID fields (e.g., `guid`)
3. Re-run import (duplicates will be merged)

**See full guide**: [AURA_SETUP.md#troubleshooting](AURA_SETUP.md#troubleshooting)

---

## Next Steps

### Immediate

1. âœ… Complete `.env` configuration (add Organization ID, Project ID)
2. âœ… Run `python scripts/test_aura_setup.py`
3. âœ… Create first import model in Aura Console
4. âœ… Test import job trigger

### Short Term

5. Set up incremental hourly pipeline for restaurant orders
6. Run historical onboarding for existing data
7. Verify data in Neo4j Browser
8. Monitor first scheduled import

### Medium Term

9. Integrate with Airflow for orchestration
10. Add monitoring and alerting
11. Implement GEPA optimization for Cypher generation
12. Set up GraphQL federation

---

## Resources

- [Neo4j Aura Console](https://console.neo4j.io)
- [Aura Import API Docs](https://neo4j.com/docs/aura/platform/api/specification/)
- [Aura CLI Docs](https://neo4j.com/labs/aura-cli/)
- [Import Service Guide](https://neo4j.com/docs/aura/import/quick-start/)
- [GPT-5 Responses API](https://platform.openai.com/docs/)

---

**Questions?** Start with `python scripts/test_aura_setup.py` to verify your setup!
